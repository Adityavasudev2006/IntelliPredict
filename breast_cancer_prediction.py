# -*- coding: utf-8 -*-
"""Breast Cancer Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kothasrivibhu/breast-cancer-prediction.5a7e0c14-d4ed-4ab0-84af-9bca0b02c78a.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251031/auto/storage/goog4_request%26X-Goog-Date%3D20251031T163237Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D67e1b8b621e94ee897d93be16a7386c30c6a70366100d54ca9aa326bd4b098e235db7e8ce82d4de70751928e9fbec5351254518b2f7527e76ff0fe56bc32621cf724386b531c0ed9e9a3e289e87dcdf76a4c68114b1401b4acb2e684d14d4d18c09d0dfc141bdbf9e0f5587260e1e4e83483ff4d5c730aed67369bcd8a168b71fb356d500d9c68b9c5be748229d3fa6fdc9859d244d340cef31cebfd673f42375804e2a54c442387142a177486ed71d9ce33398e5065e57fba885451a2ace9ac4455ceaa0997dea0aa1ba6c38d4ad341a3777456578c44d27f978def1f9cb31fcfb858a8bf38bb0e6bd29556a7046b739e124c84010eb591f7cbe8ac821cd691

## Breast Cancer Prediction

### ü©∫üéóÔ∏è Description  
Breast cancer is the most common cancer in women worldwide and the second leading cause of cancer death.  
It is diagnosed when an abnormal lump or calcium spot is detected, followed by tests to confirm if it is cancerous and has spread.  

This breast cancer dataset was obtained from the **University of Wisconsin Hospitals, Madison**, contributed by **Dr. William H. Wolberg**.  

### üìä Dataset Source: [Breast Cancer Prediction Dataset (Kaggle)](https://www.kaggle.com/datasets/merishnasuwal/breast-cancer-prediction-dataset)

#### üß≠ Structure of the Notebook

   **Introduction & Description**  
1. **Importing Libraries**  
2. **Loading the Dataset**  
3. **Analyze the Data**  
4. **Split Data into Features and Target**  
5. **Split Data into Train and Test Sets**  
6. **Modeling**  
   - 6.1 Modeling Before scaling
       - 6.1.1 Decision Tree
       - 6.1.2 Kmeans
   - 6.2 Modeling After scaling
       - 6.2.1 Scaling
       - 6.2.2 Decision tree
       - 6.2.3 Kmeans

## 1. Import Required Libraries
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split , GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score , confusion_matrix , classification_report , adjusted_rand_score

"""## 2. Read the Dataset"""

data_path = r"Breast_cancer_data.csv"
df = pd.read_csv(data_path)

df.head()

"""#### üìä Feature Descriptions

- **mean_radius**  
  The average radius of the cell nuclei, representing the approximate size of the nucleus.

- **mean_texture**  
  The average variation in gray-scale values (texture) within the cell nuclei, indicating structural irregularities.

- **mean_perimeter**  
  The average perimeter of the cell nuclei, showing how large or extended the nuclei are.

- **mean_area**  
  The average area of the cell nuclei, reflecting the overall size of the cells.

- **mean_smoothness**  
  Measures how smooth or irregular the edges of the cell nuclei are. Higher values indicate more irregular borders.

- **diagnosis**  
  The final classification of the tumor:  
  - **0** = Malignant (cancerous)  
  - **1** = Benign (non-cancerous)

## 3. Analyze the Data
"""

df.info()

df.describe()

df.columns

df['diagnosis']

"""## 4. Split Data into Features and Target"""

X = df.drop(columns=['diagnosis'])
y = df['diagnosis']

X.head()

"""## 5. Split Data into Train and Test Sets"""

X_train, X_test , y_train , y_test = train_test_split(X , y , test_size=0.3 , random_state=42)

y_train.head()

"""## 6. Modeling

### 6.1 Before Scalling

#### 6.1.1 Decision Tree
"""

clf = DecisionTreeClassifier(random_state=42)
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 3, 5, 7, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 15],
    'min_samples_leaf': [1, 2, 4, 6],
}

tree_model = GridSearchCV(clf , param_grid)

tree_model.fit(X_train , y_train)

tree_model.best_estimator_

y_pred_tree = tree_model.predict(X_test)
accuracy_score(y_test , y_pred_tree)

print(classification_report(y_test , y_pred_tree))

cm = confusion_matrix(y_test , y_pred_tree)
sns.heatmap(cm ,annot=True ,fmt='d', cmap="Reds")

"""#### 6.1.2 Kmeans"""

kmeans_model = KMeans(n_clusters=3 , random_state=42)
kmeans_model.fit(X)

y_pred_kmean = kmeans_model.predict(X)

X

adjusted_rand_score(y , y_pred_kmean)

cm = confusion_matrix(y , y_pred_kmean)
sns.heatmap(cm ,annot=True , fmt='d', cmap="Blues")

plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y_pred_kmean, s=160, alpha=0.7, cmap='viridis')
plt.title('Clustering n = 3')

"""### 6.2 After Scalling

#### 6.2.1 Scalling Data
"""

scaller = StandardScaler()
X_scaled = scaller.fit_transform(X)
X_scaled_train = scaller.fit_transform(X_train)
X_scaled_test = scaller.transform(X_test)

"""#### 6.2.2 Decision Tree"""

tree_model.fit(X_scaled_train , y_train)

tree_model.best_estimator_

y_pred_tree = tree_model.predict(X_scaled_test)
accuracy_score(y_test , y_pred_tree)

print(classification_report(y_test , y_pred_tree))

cm = confusion_matrix(y_test , y_pred_tree)
sns.heatmap(cm , fmt='d', annot=True,cmap='Blues')

"""#### 6.2.3 Kmeans"""

kmeans_model.fit(X_scaled)

y_pred_kmean = kmeans_model.predict(X_scaled)

adjusted_rand_score(y , y_pred_kmean)

from sklearn.pipeline import Pipeline

# Create a pipeline that includes scaling + model
final_model = Pipeline([
    ("scaler", StandardScaler()),
    ("classifier", DecisionTreeClassifier(
        criterion="entropy",
        max_depth=7,
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=42
    ))
])

# Train the pipeline on full data
final_model.fit(X_train, y_train)

# Save the full pipeline (model + scaler)
import pickle
with open("model.pkl", "wb") as f:
    pickle.dump(final_model, f)

print("‚úÖ Model + Scaler pipeline saved successfully as model.pkl")




